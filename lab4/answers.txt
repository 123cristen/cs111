CS111 Lab 4 
Cristen Anderson <904400797>

TODO: 
Part 2 Questions 2.2, 2.3 and Graphs

Part 1:

QUESTIONS 1.1

1) It takes a large number of iterations/threads for an error
to occur because race conditions are more likely to result in
errors the more times they occur. In this case, the more times
the global variable counter is modified by different threads,
the more likely they are to write over each other. In my tests,
I set threads to 10 and iterations to 500 to cause consistent
failure.

2) A significantly smaller number of iterations seldom fails
because it is more likely that the race conditions will not
result in an incorrect value. For example, if iterations is 1,
and there are 3 threads, imagine if thread 2 reads the value of 
counter to be zero. Before it can update the counter to 1, 
thread 2 sets the counter to 1 and then back to 0. Then 
thread 1 will correctly update counter to 1, so even though
there were race conditions the result will be correct in the
end. These types of coincidences are more likely when there
is a smaller range for counter, which happens when the number
of iterations is smaller.

QUESTIONS 1.2

1) The average cost per operation drops with increasing iterations
because because the thread creation overhead starts to become
less of a percentage of total cost. When there are just a few 
iterations, thread overhead might be half of the overall
operation cost, whereas with many iterations the thread
overhead might just be 5% of the total cost. 

2) The correct cost is the total cost minus the overhead 
of thread creation. We could figure this out by getting the clock
time before and after pthread_create, multiplying by the number
of threads and then subtracting this from the total cost. 

3) --yield operations are much slower because there is a lot
of time spent on context switching between threads. The extra
time is spent in thread overhead.

4) We cannot get valid timings if we are using yield. There
is no way to find the time spent in overhead in switching
contexts, because we do not know what thread will be switched
to and when. Because of this, we cannot find the time before
and after switching contexts in order to calculate the time
taken to do so. Without subtracting the time spent in context
switches, we cannot have a valid timing.

QUESTIONS 1.3

1) For low numbers of threads, the operations will perform similarly
because they will not have to wait as long to grab a lock or for 
compare_and_swap to return successfully. This is because there
are not as many threads trying to grab a lock simultaneously.

2) The three protected operations slow down as the thread count
rises for two different reasons. The mutex and spinlock versions
are slow because if only one thread can have the lock at a time,
it means that with many threads there is a higher demand on the
lock and more time is spent waiting to access the lock. For the
compare_and_swap version, with more threads there is higher chance
of another thread modifying counter during the same time, which
means that compare_and_swap will continue to fail for a longer 
time which causes the program to run longer.

3) Spinlocks are so expensive for large numbers of threads because
they implement busy waiting, where they take CPU resources while
waiting for the lock. With large number of threads, more time will
be spent waiting for a single lock, and thus more resources will
be used.

Part 2:

QUESTION 2.1

1) See Figure 2.1 in graphs.pdf
From 1 to 100 iterations, the average time per operations goes
down, because the thread overhead gets spread out over a larger
number of operations. We could correct for this by getting the 
clock time before and after pthread_create, multiplying by the 
number of threads and then subtracting this from the total time. 
Up from 100 iterations, the average time per operation increases 
with increasing number of iterations. This is because with 
increasing iterations, the size of the list grows, and insert 
and lookup must potentially iterate through all of the nodes. 
Therefore each insert and lookup instruction will take longer 
when there are more iterations. We could correct for this effect
by dividing the average time per operation by some value related
to the size of the list.

QUESTION 2.2

1) 

QUESTIONS 2.3

1) 
2)

Part 3:

QUESTION 3.1

1) The mutex must be held when pthread_cond_wait is called because
otherwise it is possible that the predicate condition will be
modified in between checking it and calling pthread_cond_wait. 
For example:
while(!predicate) 
	pthread_cond_wait(&condvar, &mutex)
If predicate is modified to true in between checking the condition
and calling pthread_cond_wait, then pthread_cond_wait will wait
forever because it missed the signal that predicate was changed.
We can prevent this by always having the mutex before calling
pthread_cond_wait.

2) The mutex must be released while the waiting thread is blocked
in order to allow another thread to modify the predicate condition,
since we are waiting for the predicate condition to change. Without
releasing the mutex, the predicate condition would never change and
there would be an infinite loop.

3) The mutex must be reaquired when the calling process resumes so
that the predicate condition remains the same for the code in the
next section. 
For example:
while(!predicate) 
	pthread_cond_wait(&condvar, &mutex)
// do something
In the do something section, we want to assume that the predicate
is true. In order to do that, we must have the lock on the predicate
to make sure no other threads modify it in between pthread_cond_wait
returning and the do something section.

4) This must be done inside pthread_cond_wait because otherwise
there would be a race condition. If the user released the mutex
before calling pthread_cond_wait:
while(!predicate) {
	pthread_mutex_unlock(&mutex);
	pthread_cond_wait(&condvar, &mutex)
}
Then there is a chance that another thread could modify predicate
in between pthread_mutex_unlock and pthread_cond_wait, which 
would meet pthread_cond_wait would loop forever as mentioned in
answer 1). 

5) This cannot be done in a user-mode implementation of
pthread_cond_wait, because it requires kernel actions in order
to start watching the condition variable and unlocking the
mutex in parallel. 



